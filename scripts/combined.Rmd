---
title: "Fgrandis_ad/intr"
author: "Elias"
date: "May 11, 2018"
output: html_document
---

#Fundulus grandis adapting to contamination

##Uploading files to NCBI

* Downloaded .bam files to computer
```{bash}
#log into NIH
ftp ftp-private.ncbi.nlm.nih.gov
#Go to directory
cd uploads/oziolor@gmail.com_4XM4FcAt/new_folder
#turn off interactive mode so it doesn't ask you to submit each freaking sample
prompt
#Submit all samples
mput BU*
```

##installing ANGSD
* low coverage data neutral statistics

```{bash Installing ANGSD}
#[cluster]:/data/oziolore/program/
#Installing angsd

wget http://popgen.dk/software/download/angsd/angsd0.920.tar.gz
tar xf angsd0.920.tar.gz
cd htslib;make;cd ..
cd angsd
make HTSSRC=../htslib
cd ..

```

## Took 10Mb of random regions throughout the genome and plotted coverage over them to decide what is an over-represented site threshold

```{bash Exploring high coverage}
#Getting coverage over all bases in the genome [cluster]
#starting with a merged bam of all individuals

samtools depth \
-d 10000 /home/oziolore/restoreFromData/fhet/data/align/all_merged.bam |\
gzip > /home/oziolore/restoreFromData/fhet/data/coverage/coverage_allbases.txt.gz

#Converting the high coverage data to bed file format [cluster]

zcat /home/oziolore/restoreFromData/fhet/data/coverage/coverage_allbases.txt.gz | \
awk '{OFS="\t"}{s=$2-1}{print $1,s,$2,$3}' | \
awk '{OFS="\t"}{if($4>300){print}}' | \
bedtools merge -i - -d 10 -c 4 -o count > /home/oziolore/restoreFromData/fhet/data/coverage/hicov.bed

#Getting 10Mb of random regions to makea decision on [cluster]

#!/bin/bash
dir=/home/oziolore/restoreFromData/fhet/data/coverage

zcat $dir/coverage_allbases.txt.gz | \
sort -R | \
head -n 10000000 | \
gzip > $dir/cov_10Mbrand.txt.gz

#used the following line to create a genome file in the /genome2/ subfolder
awk -v OFS='\t' {'print $1,$2'} $fai > $genome

```

## Downloading that to personal computer to use r and make decision on the threshold for high coverage

```{bash Copying coverage to computer}
#downloading from cluster
scp kodiak:/data/oziolore/fhet/data/coverage/cov_10Mbrand.txt.gz /home/elias/analysis/data/angsd/
```

```{r deciding on a threshold of high coverage}
setwd("~/analysis/data/admixture/")
cov<-read.table("~/analysis/data/angsd/cov_10Mbrand.txt.gz",header=F) #reading in output of 10Mb random bases selected
colnames(cov)<-c("chrom","pos","coverage") #assigning names to columns of data

hist(cov$coverage,breaks=1000)
hist(cov$coverage,breaks=1000,xlim=c(0,300))

subw<-cov$cov<200
hist(cov[subw,"coverage"],breaks=1000)
summary(cov$cov)
summary(cov[subw,"coverage"])

#Qualitatively I will throw out sites above 200x coverage

```

## applying filter for high coverage sites to genome file in order to only keep normal coverage ones

```{bash Creating a high coverage file excluding those SNPs}
#using awk and bedtools merge, I am using
zcat /data/oziolore/fhet/data/coverage2/coverage_allbases2.txt.gz | \
awk '{OFS="\t"}{s=$2-1}{print $1,s,$2,$3}' | \
awk '{OFS="\t"}{if($4>200){print}}' | \
/data/oziolore/program/bedtools2/bin/bedtools merge -i - -d 10 -c 4 -o count > /data/oziolore/fhet/data/coverage2/hicov.bed

#Downloading data to look at how much of the genome I threw out
scp kodiak:/data/oziolore/fhet/data/coverage2/hicov.bed /home/elias/analysis/data/angsd/
```

```{r looking at size of excluded high coverage regions}
hi<-read.table("~/analysis/data/angsd/hicov.bed",header=FALSE)
sum(hi[,4])

#Threw out 52Mb of data => 5% of the genome
```

## Creating a keepsites file with all bases that have below 200x coverage

```{bash Create the keepsites file and export to computer}
#[cluster]

zcat /data/oziolore/fhet/data/coverage2/coverage_allbases2.txt.gz | \
awk '{OFS="\t"}{s=$2-1}{print $1,s,$2,$3}' | \
awk '{OFS="\t"}{if($4<200){print}}' | \
bedtools merge -i - -d 10 > /data/oziolore/fhet/data/angsd2/keepsites2.bed

#Converting those to a .file format for ANGSD to deal with
cat /data/oziolore/fhet/data/angsd2/keepsites2.bed | \
awk '{OFS="\t"}{s=$2+1}{print $1,s,$3}' > /data/oziolore/fhet/data/angsd2/keepsites2.file

#Downloading the .bed file to make a script with 50Mb randomly selected to create expectation for SFS

scp kodiak:/data/oziolore/fhet/data/angsd2/keepsites2.bed ~/analysis/data/angsd/

```

## Script to pick up 50Mb at random from the keepsites file in order to run SAF and SFS on those for bayesian priors

```{r Selecting random 50Mb from it}

orig<-read.table("~/analysis/data/angsd/keepsites2.bed", header=F) #reading in the keepsites file

p<-(orig[,3]-orig[,2])/(sum(orig[,3]-orig[,2])) #creating probability vector so that I don't oversample large chunks

p<-unlist(p) #unlisting the vector

z<-p<0 #removing any negative probabilities for 0 values
p<-p[!z] #applying that to vector

v<-sample(x=length(p),size=11500,prob=p) #sampling 11500 chunks with probability to get out ~50Mb of the genome
v<-sort(v)
sum(orig[v,3]-orig[v,2]) #checking the total size of bases

write.table(orig[v,], file="~/analysis/data/angsd/keep50Mb.bed",row.names=FALSE,col.names=FALSE,quote=FALSE)
```

## putting that data back into [cluster]

```{bash Copy those selections to computer}
scp /home/elias/analysis/data/angsd/keep50Mb.bed kodiak:/data/oziolore/fhet/data/angsd2/

```

## Converting that to a .file

```{bash Convert from bed to file}
cat /data/oziolore/fhet/data/angsd2/keep50Mb.bed | \
awk '{OFS="\t"}{s=$2+1}{print $1,s,$3}' > /data/oziolore//fhet/data/angsd2/keep50Mb.file
```

## indexing all of those files with ANGSD

```{bash Index keepsites}
/data/oziolore/program/angsd/angsd sites index /data/oziolore/fhet/data/angsd2/keepsites2.file
/data/oziolore/program/angsd/angsd sites index /data/oziolore/fhet/data/angsd2/keep50Mb.file
```

## fixing up lists of all populations .bam files so they can be plugged into ANGSD

```{bash Fix lists in order to reflect correct directory}
cat BB2.txt | sed 's/home/data/' | sed 's/restoreFromData\///' | uniq > BB2_new.txt
```

## Starting site allele frequency estimation on a 50Mb subsample for each population (to create SFS)

```{bash Running Site Allele Frequency estimations on the 50Mb chunk}

#!/bin/bash

#PBS -l nodes=1:ppn=8
#PBS -J 1-7

pops=BB\ VB\ PB\ SJ\ BNP\ SP\ GB

one=$(echo $pops | cut -f $PBS_ARRAY_INDEX -d ' ')

#files
list=/data/oziolore/fhet/data/list2/$one\2_new.txt
genome=/data/oziolore/fhet/data/genome2/unsplit_merge.fasta
keep=/data/oziolore/fhet/data/angsd2/keep50Mb.file
outfile=/data/oziolore/fhet/data/angsd2/$one\_small
my_angsd=/data/oziolore/program/angsd/angsd

$my_angsd \
-bam $list \
-doSaf 1 \
-fold 1 \
-anc $genome \
-GL 2 \
-minMapQ 30 \
-minQ 20 \
-minind 10 \
-sites $keep \
-out $outfile

```

## Using the subsampled .saf to create site frequency spectra

```{bash Running Site Frequency Statistic on the 50Mb chunk}


#!/bin/bash

#PBS -l nodes=1:ppn=8
#PBS -J 1-7

pops=BB\ VB\ PB\ SJ\ BNP\ SP\ GB

one=$(echo $pops | cut -f $PBS_ARRAY_INDEX -d ' ')


#program and file
my_sfs=/data/oziolore/program/angsd_norm/misc/realSFS
in_saf=/data/oziolore/fhet/data/angsd2/$one\_small.saf.idx
outdir=/data/oziolore/fhet/data/angsd2
out_sfs=$one\.sfs

#code

$my_sfs $in_saf -maxIter 100 -P 8 -nSites 50000000 > $outdir/$out_sfs


```

##Creating site allele frequencies for full genome of each population

```{bash Full SAF/Not necessary step}
#!/bin/bash

#PBS -l nodes=1:ppn=8
#PBS -J 1-7

pops=BB\ VB\ PB\ SJ\ BNP\ SP\ GB

one=$(echo $pops | cut -f $PBS_ARRAY_INDEX -d ' ')

#files
list=/data/oziolore/fhet/data/list2/$one\2_new.txt
genome=/data/oziolore/fhet/data/genome2/unsplit_merge.fasta
keep=/data/oziolore/fhet/data/angsd2/keepsites2.file
outfile=/data/oziolore/fhet/data/angsd2/$one\_full
my_angsd=/data/oziolore/program/angsd/angsd

$my_angsd \
-bam $list \
-doSaf 1 \
-fold 1 \
-anc $genome \
-GL 2 \
-minMapQ 30 \
-minQ 20 \
-minind 10 \
-sites $keep \
-out $outfile
```

##Once saf and sfs are created, take sfs to R to look at distribution

```{bash Copy SFS to computer to plot}
scp kodiak:/data/oziolore/fhet/data/angsd2/*.sfs /home/elias/analysis/data/angsd/raw/
```

##Plot SFS in R
* starting with folded spectra of subsample (24 per population)

```{r Plot subsampled SFS}
sf<-list.files("~/analysis/data/angsd/subsample/","*.sfs",full.names=TRUE)
cols<-c("black","lightpink","cadetblue3","grey80","firebrick2","cadetblue1","grey40")
pop<-list("bb","bnp","gb","pb","sj","sp","vb")

for(i in 1:7){
  pop[[i]]<-scan(sf[[i]])
}

par(mfrow=c(3,3),mar=c(2,2,2,2))
for(i in 1:7){
  plot(log(pop[[i]]),col=cols[i],pch=20,lwd=3)
}
```

* unfolded spectra of subsample (24 per population)

```{r Plot unfolded subsampled SFS}
sf<-list.files("~/analysis/data/angsd/subsample/unfolded/","*.sfs",full.names=TRUE)
cols<-c("black","lightpink","cadetblue3","grey80","firebrick2","cadetblue1","grey40")
pop<-list("bb","bnp","gb","pb","sj","sp","vb")

for(i in 1:7){
  pop[[i]]<-scan(sf[[i]])
}

par(mfrow=c(3,3),mar=c(2,2,2,2))
for(i in 1:7){
  plot(log(pop[[i]]),col=cols[i],pch=20,lwd=3)
}
```

* folded spectra of all individuals

```{r Plot full sample SFS}
sf<-list.files("~/analysis/data/angsd/raw/","*.sfs",full.names=TRUE)
cols<-c("black","lightpink","cadetblue3","grey80","firebrick2","cadetblue1","grey40")
pop<-list("bb","bnp","gb","pb","sj","sp","vb")

for(i in 1:7){
  pop[[i]]<-scan(sf[[i]])
}

par(mfrow=c(3,3),mar=c(2,2,2,2))
for(i in 1:7){
  plot(log(pop[[i]]),col=cols[i],pch=20,lwd=3)
}
```

* unfolded spectra of all individuals

```{r Plot full sample unfolded SFS}
sf<-list.files("~/analysis/data/angsd/raw/unfolded/","*.sfs",full.names=TRUE)
cols<-c("black","lightpink","cadetblue3","grey80","firebrick2","cadetblue1","grey40")
pop<-list("bb","bnp","gb","pb","sj","sp","vb")

for(i in 1:7){
  pop[[i]]<-scan(sf[[i]])
}

par(mfrow=c(3,3),mar=c(2,2,2,2))
for(i in 1:7){
  plot(log(pop[[i]]),col=cols[i],pch=20,lwd=3)
}
```

##-doThetas seqfault error again
* that has been throwing segfaults left and right for different people.
* fix is to change lines in abcSaf.cpp that have:
r->pLikes[myCounter] =new float[numInds+1]
to
r->pLikes[myCounter] =new float[2*numInds+1]
* will only use for folded frequency spectra estimations. For unfolded, leave original in /program/angsd_norm/angsd

##Everything works fine on -doSaf unfolded spectra

##Re-do of SFS for 1 pop (looks like crap)

```{bash Re-do of crappy SFS}
#!/bin/bash

#program and file
my_sfs=/data/oziolore/program/angsd_norm/misc/realSFS
in_saf=/data/oziolore/fhet/data/angsd2/BB_small.saf.idx
outdir=/data/oziolore/fhet/data/angsd2
out_sfs=BB.sfs

#code

$my_sfs $in_saf -maxIter 100 -P 8 -nSites 50000000 > $outdir/$out_sfs

```

##Reading thetas.gz file to a readable state: below is command for folded estimate of subsample

```{bash Making the thetas estimates into a readable file}
#!/bin/bash

#PBS -l nodes=1:ppn=8
#PBS -l walltime=24:00:00
#PBS -J 1-7

pops=BB\ VB\ PB\ SJ\ BNP\ SP\ GB

one=$(echo $pops | cut -f $PBS_ARRAY_INDEX -d ' ')


#program/file
my_stat=/data/oziolore/program/angsd_norm/misc/thetaStat
file=/data/oziolore/fhet/data/angsd2/theta/subsample/$one\.theta.thetas.idx
out=/data/oziolore/fhet/data/angsd2/theta/subsample/$one\_readable_theta.gz

$my_stat print $file | gzip > $out
```

* SP has some issues in one of the characters in the compressed readable version. re-doing
    + worked after just repeating script
    
* Breaking them into sliding windows

```{bash Sliding windows from Noah's lift file UPDATE THIS WITH HOW YOU CREATED THE WINDOWS}
#!/bin/bash

#PBS -l nodes=1:ppn=8
#PBS -l walltime=02:00:00
#PBS -J 1-7

pops=BB\ VB\ PB\ SJ\ BNP\ SP\ GB

one=$(echo $pops | cut -f $PBS_ARRAY_INDEX -d ' ')

#programs and files

my_bedtools=/data/oziolore/program/bedtools2/bin/bedtools
thetas=/data/oziolore/fhet/data/angsd2/theta/$one\_readable_theta.gz
window=/data/oziolore/fhet/data/windows2/new_noah.1kb.bed
my_genome=/data/oziolore/fhet/data/genome2/unsplit_merge.fasta.fai
outdir=/data/oziolore/fhet/data/angsd2/theta          
outfile=$one\_neut_1kb.bed

zcat $thetas | \
egrep -v "^#" | \
awk '{OFS="\t"}{w=exp($3)}{pi=exp($4)}{s=$2-1}{print $1,s,$2,w,pi}' | \
$my_bedtools map \
-a $window \
-b stdin \
-g <(cut -f 1-2 $my_genome) \
-c 4,4,5,5 \
-o sum,count,sum,count > $outdir/$outfile

```
    
## Plotting distribution of neutral estimates (full sample size)
* starting with folded samples

```{bash Copy those files into computer}
scp kodiak:/data/oziolore/fhet/data/angsd2/theta/*_1kb* /home/elias/analysis/data/angsd/raw/

```

* Break the file into pi, theta and and calculate tajima's D
* starting with 1kb windows by noah's lift file. Merging them into 20 kb windows

```{r First data wrangling with R}
###Buffalo Bayou loding pi(column6), theta(column4), and counts(columns 5,7)

bb<-read.table("~/analysis/data/angsd/raw/BB_neut_1kb.bed",stringsAsFactors=FALSE) #reading in all files
vb<-read.table("~/analysis/data/angsd/raw/VB_neut_1kb.bed",stringsAsFactors=FALSE)
pb<-read.table("~/analysis/data/angsd/raw/PB_neut_1kb.bed",stringsAsFactors=FALSE)
sj<-read.table("~/analysis/data/angsd/raw/SJ_neut_1kb.bed",stringsAsFactors=FALSE)
bnp<-read.table("~/analysis/data/angsd/raw/BNP_neut_1kb.bed",stringsAsFactors=FALSE)
sp<-read.table("~/analysis/data/angsd/raw/SP_neut_1kb.bed",stringsAsFactors=FALSE)
gb<-read.table("~/analysis/data/angsd/raw/GB_neut_1kb.bed",stringsAsFactors=FALSE)

colnam<-c("scaf","Start","End","Theta","Tcount","Pi","Pcount") #giving them column names
colnames(bb)<-colnam
colnames(vb)<-colnam
colnames(pb)<-colnam
colnames(sj)<-colnam
colnames(bnp)<-colnam
colnames(sp)<-colnam
colnames(gb)<-colnam

#Slide function----
#This basically takes width number of windows and takes an average pi of them
slide<-function(x,y,width,slide){
  aver<-c()
  for(i in 1:length(y)){
    count=1
    if(is.na(y[i])){added=0}else{added<-y[i]}
    for(j in 1:(width-1)){
      index<-sum(i,j)
      if(is.na(x[index,1])){next()}
      if(is.na(y[index])){next()}
      if(x[index-1,1]==x[index,1]){
        added=added+y[index]
        count=count+1
      }else{next()}
    }
    aver[i]<-added
  }
  return(aver)
}

#Slidecount function----
slidecount<-function(x,y,width,slide){
  count<-c()
  for(i in 1:length(y)){
    if(is.na(y[i])){added=0}else{added<-y[i]}
    for(j in 1:(width-1)){
      index<-sum(i,j)
      if(is.na(x[index,1])){next()}
      if(is.na(y[index])){next()}
      if(x[index-1,1]==x[index,1]){
        added=added+y[index]
      }else{next()}
    }
    count[i]<-added
  }
  return(count)
}

pops<-list(bb,vb,pb,sj,bnp,sp,gb)
popnames<-c("bb","vb","pb","sj","bnp","sp","gb")
names(pops)<-popnames
pops2<-pops

for(i in popnames){ #converting files into 20kb merged windows
  for(j in c(4,6)){
    print(i)
    pops2[[i]][,j]<-slide(pops[[i]],as.numeric(pops[[i]][,j]),width=20,slide=1)
  }
  for(f in c(5,7)){
    print(i)
    pops2[[i]][,f]<-slidecount(pops[[i]],as.numeric(pops[[i]][,f]),width=20,slide=1)
  }
}


write.table(pops2[["bb"]],"~/analysis/data/angsd/raw/BB_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")
write.table(pops2[["vb"]],"~/analysis/data/angsd/raw/VB_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")
write.table(pops2[["pb"]],"~/analysis/data/angsd/raw/PB_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")
write.table(pops2[["sj"]],"~/analysis/data/angsd/raw/SJ_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")
write.table(pops2[["bnp"]],"~/analysis/data/angsd/raw/BNP_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")
write.table(pops2[["sp"]],"~/analysis/data/angsd/raw/SP_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")
write.table(pops2[["gb"]],"~/analysis/data/angsd/raw/GB_neut2.bed",quote = FALSE,row.names = FALSE,col.names = FALSE,sep=",")

```

*Let's read in the windowed estimates wejust created

```{r}

bb<-read.csv("~/analysis/data/angsd/raw/BB_neut2.bed",stringsAsFactors=FALSE) #reading in all files
vb<-read.csv("~/analysis/data/angsd/raw/VB_neut2.bed",stringsAsFactors=FALSE)
pb<-read.csv("~/analysis/data/angsd/raw/PB_neut2.bed",stringsAsFactors=FALSE)
sj<-read.csv("~/analysis/data/angsd/raw/SJ_neut2.bed",stringsAsFactors=FALSE)
bnp<-read.csv("~/analysis/data/angsd/raw/BNP_neut2.bed",stringsAsFactors=FALSE)
sp<-read.csv("~/analysis/data/angsd/raw/SP_neut2.bed",stringsAsFactors=FALSE)
gb<-read.csv("~/analysis/data/angsd/raw/GB_neut2.bed",stringsAsFactors=FALSE)

colnam<-c("scaf","Start","End","Theta","Tcount","Pi","Pcount") #giving them column names
colnames(bb)<-colnam
colnames(vb)<-colnam
colnames(pb)<-colnam
colnames(sj)<-colnam
colnames(bnp)<-colnam
colnames(sp)<-colnam
colnames(gb)<-colnam

pops<-list(bb,vb,pb,sj,bnp,sp,gb)
popnames<-c("bb","vb","pb","sj","bnp","sp","gb")
names(pops)<-popnames

cov<-cbind(bb[1:3],bb[,7],vb[,7],pb[,7],sj[,7],bnp[,7],sp[,7],gb[,7]) #creating a vector of coverage of each statistic call

for(i in popnames){
  for(j in 4:7){
  pops[[i]][,j]<-as.numeric(pops[[i]][,j])
  }
}


###Calculating blocks of certianty
nsnps<-cov[,4] #creating a vector that sums all calls over each SNP
for (i in 5:10){
  nsnps <- nsnps + cov[,i]
}
nsnps <- nsnps/7 #dividing by number of populations and only using sites that have at least 20 SNPS used over the window per population

subw <- nsnps > 50 #filter to be saved for these windows

#calculating mean and median pi----
pimean<-c() #calculating mean/something is wrong here. check
for(i in popnames){
  pimean[i]<-sum(pops[[i]][subw,6],na.rm=TRUE)/sum(pops[[i]][subw,7],na.rm=TRUE)
}

popbase<-list()

for(i in popnames){ #calculating per base estimates rather than averaged estimates over the windows we have
  popbase[[i]]<-cbind(pops[[i]][,1:3],pops[[i]][,4]/pops[[i]][,5],pops[[i]][,6]/pops[[i]][,7])
}

names<-c("scaf", "start","end",'theta/b',"pi/b")

for(i in popnames){ #giving the new list column names
  colnames(popbase[[i]])<-names
}

wt<-matrix(nrow=length(popbase[[1]][,1]),ncol=length(popnames)) #making a matrix of Waterson's theta values
for(i in 1:7){
  wt[,i]<-popbase[[i]][,4]
}
colnames(wt)<-popnames

pi<-matrix(nrow=length(popbase[[1]][,1]),ncol=length(popnames)) #making a matrix of pi values
for(i in 1:7){
  pi[,i]<-popbase[[i]][,5]
}
colnames(pi)<-popnames


source("~/analysis/scripts/angsd/tajimas.r")

###Before calculating statistics I want to merge these into 5kb windows with 1kb slide.

taj<-list()

for(i in popnames){ #calculating tajima's D for all populations through a function in r; takes forever
  print(i)
  for(j in 1:dim(pi)[[1]]){
    taj[[i]]<-c(taj[[i]],tajimas(pi[j,i],wt[j,i],24))
  }
}


taj<-cbind(popbase[[1]][,1:3],taj[["bb"]],taj[["vb"]],taj[["pb"]],taj[["sj"]],taj[["bnp"]],taj[["sp"]],taj[["gb"]]) #binding into a dataframe

taj<-cbind(taj,keep=as.numeric(subw)) #keeping the filter of low representation bases

tajname<-c("scaf","start","end","bb","vb","pb","sjsp","bnp",
            "sp","gb","keep") #column names
colnames(taj)<-tajname

write.csv(taj,file="~/analysis/data/angsd/taj",quote=FALSE,row.names=FALSE) #writing tajima's d

theta<-cbind(popbase[["bb"]][1:3],wt,keep=as.numeric(subw))

thetname<-c("scaf","start","end","bb","vb","pb","sjsp","bnp",
            "sp","gb","keep")
colnames(theta)<-thetname

write.csv(theta,file="~/analysis/data/angsd/thetas",quote=FALSE,row.names=FALSE)


pi<-cbind(popbase[["bb"]][1:3],pi,keep=as.numeric(subw))

piname<-c("scaf","start","end","bb","vb","pb","sj","bnp","sp","gb","keep")
colnames(pi)<-piname
write.csv(pi,file="~/analysis/data/angsd/pi",quote=FALSE,row.names=FALSE)

write.csv(cov,file="~/analysis/data/angsd/cov",quote=FALSE,row.names = FALSE)

```

```{r}
library('tidyr')
library('tibble')
library('magrittr')
library('dplyr')

#loading neutrality stats----

theta<-read.table("~/analysis/data/angsd/thetas",header=TRUE, sep=',') #reading in summary statistics
pi<-read.table("~/analysis/data/angsd/pi", header=TRUE, sep=',')
taj<-read.table("~/analysis/data/angsd/taj",header=TRUE, sep=',')

subw<-pi[,"keep"]>0 #applying filter of low coverage

##ggplot pi----
library(ggplot2)
library(reshape2)

mpi<-melt(pi[,1:10],id=c("scaf","start","end"))

ggplot(mpi,
       aes(x=variable,y=value,fill=variable,color=variable))+
  geom_violin(trim=FALSE,draw_quantiles = 0.5,lwd=2)+
  scale_fill_manual(values=c("black","grey40","grey80","firebrick2","lightpink","cadetblue1","cadetblue3"))+
  scale_color_manual(values=c("grey40",rep("black",6)))+
  scale_y_continuous(limits=c(0,.015))+
  theme_classic()+
  labs(y="",x="")+
  theme(axis.line.y=element_line(color="black",size=5),axis.line=element_line(color="black",size=5))+
  theme(axis.text.y=element_text(color="black",size=40))

mtaj<-melt(taj[,1:10],id=c("scaf","start","end"))

ggplot(mtaj,
       aes(x=variable,y=value,fill=variable,color=variable))+
  geom_violin(trim=FALSE,draw_quantiles = 0.5,lwd=2)+
  scale_fill_manual(values=c("black","grey40","grey80","firebrick2","lightpink","cadetblue1","cadetblue3"))+
  scale_color_manual(values=c("grey40",rep("black",6)))+
  scale_y_continuous(limits=c(-.35,.1))+
  theme_classic()+
  labs(y="",x="")+
  theme(axis.line.y=element_line(color="black",size=5),axis.line=element_line(color="black",size=5))+
  theme(axis.text.y=element_text(color="black",size=40))

#visualizing pi distribution----

bbp<-density(pi[subw,4],na.rm=TRUE)
vbp<-density(pi[subw,5],na.rm=TRUE)
pbp<-density(pi[subw,6],na.rm=TRUE)
sjp<-density(pi[subw,7],na.rm=TRUE)
bnpp<-density(pi[subw,8],na.rm=TRUE)
spp<-density(pi[subw,9],na.rm=TRUE)
gbp<-density(pi[subw,10],na.rm=TRUE)

par(mfrow=c(2,1),mar=c(4,5,2,2),mgp=c(3,2,0))

plot(gbp,xlim=c(0.0001,.025),col="cadetblue3",bty="l",ylim=c(0,450),
     cex.lab=2,xlab="",ylab="",lwd=3,main="",cex.axis=3)
#polygon(gbp,col="cadetblue1",density=100,border=NA)
lines(spp,xlim=c(0.001,.025),col="cadetblue3",lwd=3)
#polygon(spp,col="cadetblue3",density=100,border=NA)
lines(bnpp,xlim=c(0.001,.025),col="firebrick2",lwd=3)
#polygon(bnpp,col="lightpink",density=100,border=NA)
lines(sjp,xlim=c(0.001,.025),col="firebrick2",lwd=3)
#polygon(sjp,col="firebrick2",density=100,border=NA)
lines(pbp,xlim=c(0.001,.025),col="black",lwd=3)
#polygon(pbp,col="grey80",density=100,border=NA)
lines(vbp,xlim=c(0.001,.025),col="black",lwd=3)
#polygon(vbp,col="grey40",density=100,border=NA)
lines(bbp,xlim=c(0.001,.025),col="black",lwd=3)
#polygon(bbp,col="black",density=100,border=NA)



box(lwd=7,bty="l")

#visualizing tajima's D distribution----

bbtaj<-density(taj[subw,4],na.rm=TRUE)
vbtaj<-density(taj[subw,5],na.rm=TRUE)
pbtaj<-density(taj[subw,6],na.rm=TRUE)
sjtaj<-density(taj[subw,7],na.rm=TRUE)
bnptaj<-density(taj[subw,8],na.rm=TRUE)
sptaj<-density(taj[subw,9],na.rm=TRUE)
gbtaj<-density(taj[subw,10],na.rm=TRUE)

#par(mfrow=c(1,1),mgp=c(3,2,0))
plot(bnptaj,xlim=c(-.3,.3),col="firebrick2",bty="l",
     ylim=c(0,50),xlab="",ylab="",main="",lwd=3,cex.axis=4)
#polygon(bnptaj,col="lightpink",density=100,border=NA)
lines(sjtaj,xlim=c(-.15,.3),col="firebrick2",lwd=3)
#polygon(sjtaj,col="red",density=100,border=NA)
lines(pbtaj,xlim=c(-.15,.3),col="black",lwd=3)
#polygon(pbtaj,col="grey80",density=100,border=NA)
lines(vbtaj,xlim=c(-.15,.3),col="black",lwd=3)
#polygon(vbtaj,col="grey40",density=100,border=NA)
lines(bbtaj,xlim=c(-.15,.3),col="black",lwd=3)
#polygon(bbtaj,col="black",density=100,border=NA)
#polygon(sptaj,col="cadetblue3",density=100,border=NA)
lines(sptaj,xlim=c(-.15,.3),col="cadetblue3",lwd=3)
lines(gbtaj,xlim=c(-.15,.3),col="cadetblue3",lwd=3)
#polygon(gbtaj,col="cadetblue1",density=100,border=NA)

box(lwd=7,bty="l")

# 
# #####Plotting theta vs pi----
# 
# par(mfrow=c(3,3))
# plot(theta[,4],pi[,4],pch=20,cex=.5,col="black")
# abline(a=0,b=1,col="red")
# plot(theta[,5],pi[,5],pch=20,cex=.5,col="grey")
# abline(a=0,b=1,col="red")
# plot(theta[,6],pi[,6],pch=20,cex=.5,col="red")
# abline(a=0,b=1,col="black")
# plot(theta[,7],pi[,7],pch=20,cex=.5,col="orange")
# abline(a=0,b=1,col="red")
# plot(theta[,8],pi[,8],pch=20,cex=.5,col="yellow")
# abline(a=0,b=1,col="red")
# plot(theta[,9],pi[,9],pch=20,cex=.5,col="green")
# abline(a=0,b=1,col="red")
# plot(theta[,10],pi[,10],pch=20,cex=.5,col="blue")
# abline(a=0,b=1,col="red")
```